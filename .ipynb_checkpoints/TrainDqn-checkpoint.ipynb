{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from collections import deque\n",
    "import tensorflow as tf\n",
    "from dqn_agent import DQNAgent\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# setting seeds for result reproducibility. This is not super important\n",
    "random.seed(1024)\n",
    "np.random.seed(1024)\n",
    "tf.compat.v1.random.set_random_seed(1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainDqn:\n",
    "\n",
    "    def __init__(self):\n",
    "        # Hyperparameters / Constants\n",
    "        self.noOfEpisodes = 1000\n",
    "        self.ReplayMemoryQueueSize = 100000\n",
    "        self.minReplayMemoryQueueSize = 10000\n",
    "        self.sampleBatchSize = 1000\n",
    "        self.epsilon = 1\n",
    "        self.epsilonDecay = 0.99\n",
    "        self.minEpsilon = 0.001\n",
    "        self.discount = 0.99\n",
    "        self.doRender = False\n",
    "        self.gameEnv = 'MountainCar-v0'\n",
    "\n",
    "        # Environment details\n",
    "        self.env = gym.make(self.gameEnv)\n",
    "        self.actionDimension = self.env.action_space.n\n",
    "        self.observationDimension = self.env.observation_space.shape\n",
    "\n",
    "        # creating own session to use across all the Keras/Tensorflow models we are using\n",
    "        self.sess = tf.compat.v1.Session()\n",
    "\n",
    "        # Replay memory to store experiances of the model with the environment\n",
    "        self.replay_memory = deque(maxlen=self.ReplayMemoryQueueSize)\n",
    "\n",
    "        # Our models to solve the mountaincar problem.\n",
    "        self.agent = DQNAgent(self.sess, self.actionDimension, self.observationDimension)\n",
    "\n",
    "\n",
    "    def train_dqn_agent(self):\n",
    "        minibatch = random.sample(self.replay_memory, self.sampleBatchSize)\n",
    "        currStates = []\n",
    "        nextStates = []\n",
    "        for index, sample in enumerate(minibatch):\n",
    "            currState, action, reward, nextState, done = sample\n",
    "            currStates.append(currState)\n",
    "            nextStates.append(nextState)\n",
    "    \n",
    "        currStates = np.array(currStates)\n",
    "        nextStates = np.array(nextStates)\n",
    "    \n",
    "        # action values for the currStates\n",
    "        currActionValues = self.agent.model.predict(currStates)\n",
    "        # action values for the nextStates taken from our agent (Q network)\n",
    "        nextActionValues = self.agent.model.predict(nextStates)\n",
    "        for index, sample in enumerate(minibatch):\n",
    "            currState, action, reward, nextState, done = sample\n",
    "            if not done:\n",
    "                # Q(st, at) = rt + discount * max(Q(s(t+1), a(t+1)))\n",
    "                currActionValues[index][action] = reward + self.discount * np.amax(nextActionValues[index])\n",
    "            else:\n",
    "                # Q(st, at) = rt\n",
    "                currActionValues[index][action] = reward\n",
    "        # train the agent with new Q values for the states and the actions\n",
    "        self.agent.model.fit(currStates, currActionValues, verbose=0)\n",
    "\n",
    "    def StartPlaying(self):\n",
    "        max_reward = -1000\n",
    "        for episode in range(self.noOfEpisodes):\n",
    "            currState = self.env.reset()\n",
    "            done = False\n",
    "            episodeReward = 0\n",
    "            episodeLength = 0\n",
    "            while not done:\n",
    "                episodeLength += 1\n",
    "                # set doRender = True if want to see agent while training. But makes training a bit slower.\n",
    "                if self.doRender:\n",
    "                    self.env.render()\n",
    "\n",
    "                if(np.random.uniform(0, 1) < self.epsilon):\n",
    "                    # Take random action\n",
    "                    action = np.random.randint(0, self.actionDimension)\n",
    "                else:\n",
    "                    # Take action that maximizes the total reward\n",
    "                    action = np.argmax(self.agent.model.predict(np.expand_dims(currState, axis=0))[0])\n",
    "\n",
    "                nextState, reward, done, _ = self.env.step(action)\n",
    "\n",
    "                episodeReward += reward\n",
    "\n",
    "                if done and episodeLength < 200:\n",
    "                    # If episode is ended the we have won the game. So, give some large positive reward\n",
    "                    reward = 250 + episodeReward\n",
    "                    episodeReward = reward\n",
    "                    # save the model if we are getting maximum score this time\n",
    "                    if(episodeReward > max_reward):\n",
    "                        self.agent.model.save_weights(str(episodeReward)+\"_agent_.h5\")\n",
    "                else:\n",
    "                    # In other cases reward will be proportional to the distance that car has travelled \n",
    "                    # from it's previous location + velocity of the car\n",
    "                    reward = 3*abs(nextState[0] - currState[0]) + 5*abs(currState[1])\n",
    "            \n",
    "                # Add experience to replay memory buffer\n",
    "                self.replay_memory.append((currState, action, reward, nextState, done))\n",
    "                currState = nextState\n",
    "        \n",
    "                if(len(self.replay_memory) < self.minReplayMemoryQueueSize):\n",
    "                    continue\n",
    "        \n",
    "                self.train_dqn_agent()\n",
    "\n",
    "\n",
    "            if(self.epsilon > self.minEpsilon and len(self.replay_memory) > self.minReplayMemoryQueueSize):\n",
    "                self.epsilon *= self.epsilonDecay\n",
    "\n",
    "            # some bookkeeping.\n",
    "            max_reward = max(episodeReward, max_reward)\n",
    "            print('Episode', episode, 'Episodic Reward', episodeReward, 'Maximum Reward', max_reward, 'epsilon', self.epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj = TrainDqn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj.StartPlaying()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
